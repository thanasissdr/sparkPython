{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "#from pyspark.sql.functions import avg, mean, stddev, udf, col, min, max, round\n",
    "from pyspark.sql.types import BooleanType, BinaryType, DoubleType, FloatType, IntegerType\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get/Set spark attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.67:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_uPdAtEd</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112b6e9b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()\n",
    "\n",
    "conf_list = [('spark.app.name', 'Spark_uPdAtEd')]\n",
    "conf = spark.sparkContext._conf.setAll(conf_list)\n",
    "spark.sparkContext.stop()\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: string (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./telco-customer-churn.csv\"\n",
    "\n",
    "df = spark.read.csv(filepath, header=True)\n",
    "\n",
    "print((df.columns))\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(type_, *cols):\n",
    "    global df\n",
    "    \n",
    "    for c in cols:\n",
    "        df = df.withColumn(c, f.col(c).cast(type_))\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"MonthlyCharges\", \"TotalCharges\"]\n",
    "df = change_type(FloatType(), *columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries using the Spark Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|stddev_samp(MonthlyCharges)|\n",
      "+---------------------------+\n",
      "|          30.09004712627172|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.stddev(\"MonthlyCharges\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+------------------+\n",
      "|gender|      monthly_avg|         total_avg|\n",
      "+------+-----------------+------------------+\n",
      "|Female|65.20424314321728|2283.1909871540843|\n",
      "|  Male|64.32748243651142| 2283.407860119869|\n",
      "+------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df = df.groupBy(\"gender\").agg(f.mean(\"MonthlyCharges\").alias(\"monthly_avg\"), f.mean(\"TotalCharges\").alias(\"total_avg\"))\\\n",
    "                            .orderBy(\"total_avg\", ascending=True)\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------+\n",
      "|gender|minimumTotalCharges|maximum|\n",
      "+------+-------------------+-------+\n",
      "|  Male|               18.8| 8684.8|\n",
      "|Female|               18.9|8672.45|\n",
      "+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").agg(f.round(f.min(\"TotalCharges\"), 1).alias(\"minimumTotalCharges\"), f.max(\n",
    "    \"TotalCharges\").alias(\"maximum\")).sort(\"gender\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries using the SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='telco', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(name=\"telco\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|gender|number_of_people|\n",
      "+------+----------------+\n",
      "|Female|            3488|\n",
      "|  Male|            3555|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT gender, COUNT(gender) AS number_of_people FROM telco GROUP BY gender\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|gender|rounded|maximum|\n",
      "+------+-------+-------+\n",
      "|Female|   18.9|8672.45|\n",
      "|  Male|   18.8| 8684.8|\n",
      "+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT gender, ROUND(MIN(TotalCharges), 1) AS rounded, MAX(TotalCharges) AS maximum FROM telco GROUP BY gender\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fetching only the rows where column B has the maximum value if we PARTITION BY A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  5|  v|\n",
      "|  a|  5|  v|\n",
      "|  a|  8|  v|\n",
      "|  e|  7|  v|\n",
      "|  b|  1|  c|\n",
      "|  b|  3|  c|\n",
      "|  c|  3|  c|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('a', 5, 'v'),\n",
    "    ('a', 5, 'v'),\n",
    "    ('a', 8, 'v'),\n",
    "    ('e', 7, 'v'),\n",
    "    ('b', 1, \"c\"),\n",
    "    ('b', 3, \"c\"),\n",
    "    ('c', 3, \"c\")\n",
    "]\n",
    "df2 = spark.createDataFrame(data, [\"A\", \"B\", \"C\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  8|  v|\n",
      "|  b|  3|  c|\n",
      "|  c|  3|  c|\n",
      "|  e|  7|  v|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"A\")\n",
    "\n",
    "df2.withColumn(\"maxB\", f.max(\"B\").over(w)).where(f.col(\"maxB\") == f.col(\"B\")).drop(f.col(\"maxB\")).sort(f.col(\"A\"), ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+\n",
      "|  A|  B|  C|maxB|\n",
      "+---+---+---+----+\n",
      "|  e|  7|  v|   7|\n",
      "|  c|  3|  c|   3|\n",
      "|  b|  3|  c|   3|\n",
      "|  a|  8|  v|   8|\n",
      "+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.registerTempTable(\"example\") # df2.createOrReplaceTempView(\"example\")\n",
    "\n",
    "query = \"SELECT * FROM (SELECT *, MAX(B) OVER (PARTITION BY A) AS maxB FROM example) WHERE B=maxB\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+\n",
      "|  A|  B|  C|maxB|\n",
      "+---+---+---+----+\n",
      "|  e|  7|  v|   7|\n",
      "|  c|  3|  c|   3|\n",
      "|  b|  3|  c|   3|\n",
      "|  a|  8|  v|   8|\n",
      "+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"A\")\n",
    "df2.withColumn(\"maxB\", f.max(\"B\").over(w)).where(f.col(\"B\") == f.col(\"maxB\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a custom function to use in SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+\n",
      "|  A|  B|  C|squared_B|\n",
      "+---+---+---+---------+\n",
      "|  a|  5|  v|     25.0|\n",
      "|  a|  5|  v|     25.0|\n",
      "|  a|  8|  v|     64.0|\n",
      "|  e|  7|  v|     49.0|\n",
      "|  b|  1|  c|      1.0|\n",
      "|  b|  3|  c|      9.0|\n",
      "|  c|  3|  c|      9.0|\n",
      "+---+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "spark.udf.register(\"SQUARE\", square, returnType=FloatType())\n",
    "\n",
    "\n",
    "df2.withColumn('squared_B', square(f.col(\"B\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Crosstab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "|A_B|  1|  3|  5|  7|  8|\n",
      "+---+---+---+---+---+---+\n",
      "|  e|  0|  0|  0|  1|  0|\n",
      "|  b|  1|  1|  0|  0|  0|\n",
      "|  a|  0|  0|  2|  0|  1|\n",
      "|  c|  0|  1|  0|  0|  0|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.crosstab(col1='A', col2='B').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Coalesce` / `Repartition`\n",
    "\n",
    "- Coalesce can **only** minimize the number of partitions, in contrast with repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|color|\n",
      "+---+-----+\n",
      "| 10| blue|\n",
      "| 13|  red|\n",
      "| 15| blue|\n",
      "| 99|  red|\n",
      "| 67| blue|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(10, \"blue\"),\n",
    "  (13, \"red\"),\n",
    "  (15, \"blue\"),\n",
    "  (99, \"red\"),\n",
    "  (67, \"blue\")]\n",
    "df = spark.createDataFrame(data=data).toDF('age', 'color')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(age=10, color='blue')],\n",
       " [Row(age=13, color='red')],\n",
       " [Row(age=15, color='blue')],\n",
       " [Row(age=99, color='red'), Row(age=67, color='blue')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_df = df.coalesce(2)\n",
    "div_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(age=10, color='blue'), Row(age=13, color='red')],\n",
       " [Row(age=15, color='blue'),\n",
       "  Row(age=99, color='red'),\n",
       "  Row(age=67, color='blue')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(age=13, color='red'), Row(age=99, color='red')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(age=10, color='blue'), Row(age=15, color='blue'), Row(age=67, color='blue')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Default number of partitions 200\n",
    "print(df.repartition('color').rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $2\\times 10^9$ rows split into $13,000$ partitions, but we need to take only $2,000$ random rows. If we sample, we will still have $13,000$ partitions, so most of the partitions will be empty. Thus, we need to repartition the sample.\n",
    "\n",
    "In general:\n",
    "`number_of_partitions = number_of_CPUs in the cluster * 2, 3, 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why did we use the repartition method instead of coalesce?**\n",
    "\n",
    "A full data shuffle is an expensive operation for large data sets, but our data puddle is only 2,000 rows. The repartition method returns equal sized text files, which are more efficient for downstream consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
