{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import string\n",
    "\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import mean, avg, stddev, udf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import cycle\n",
    "from operator import add, mul\n",
    "\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext (RDD)\n",
    "\n",
    "- filter, foreach, distinct, first, reduce\n",
    "- map, mapValues, flatMap, flatMapValues, collectAsMap, mapPartitions, mapPartitionsWithSplit\n",
    "- groupBy, groupByKey, reduceByKey, countByKey, combineByKey, foldByKey, cogroup\n",
    "- glom\n",
    "- cartesian\n",
    "- count, countByValue\n",
    "- take\n",
    "- join, union, rightOuterJoin, leftOuterJoin\n",
    "- pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]\n"
     ]
    }
   ],
   "source": [
    "n = zip(cycle(['even', 'odd']), np.arange(10))\n",
    "numbers = sc.parallelize(n)\n",
    "print(numbers.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('even', 0), ('odd', 1)],\n",
       " [('even', 2), ('odd', 3)],\n",
       " [('even', 4), ('odd', 5)],\n",
       " [('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numbers.getNumPartitions())\n",
    "numbers.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', -54), ('odd', -70)]\n",
      "[('even', 0), ('odd', 945)]\n",
      "[('even', 0), ('odd', 15120)]\n"
     ]
    }
   ],
   "source": [
    "print(numbers.aggregateByKey(seqFunc=lambda x, y: x * y,\n",
    "                      \n",
    "                            combFunc=lambda x, y: x-y, zeroValue=1).collect())\n",
    "\n",
    "#print(numbers.combineByKey(createCombiner=lambda x: [x], mergeCombiners=lambda x, y: x-y, mergeValue=lambda x,y: x*y).collect())\n",
    "\n",
    "print(numbers.reduceByKey(func=lambda x, y: x * y).collect())\n",
    "print(numbers.foldByKey(zeroValue=2, func=lambda x, y: x * y ).collect()) # (2*1) * (2*3) * (2*5) * (2*7*9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', [0, 2, 4, 6, 8]), ('odd', [1, 3, 5, 7, 9])]\n"
     ]
    }
   ],
   "source": [
    "print(numbers.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'even': 5, 'odd': 5})\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(numbers.countByKey())\n",
    "print(numbers.countByKey()['even'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map / flatMap / mapValues / flatMapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd']\n",
      "\n",
      "\n",
      "['e', 'v', 'e', 'n', 'o', 'd', 'd', 'e', 'v', 'e', 'n', 'o', 'd', 'd', 'e', 'v', 'e', 'n', 'o', 'd', 'd', 'e', 'v', 'e', 'n', 'o', 'd', 'd', 'e', 'v', 'e', 'n', 'o', 'd', 'd']\n",
      "\n",
      "\n",
      "[('even', 0), ('odd', 1), ('even', 4), ('odd', 9), ('even', 16), ('odd', 25), ('even', 36), ('odd', 49), ('even', 64), ('odd', 81)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('even', 0),\n",
       " ('even', 2),\n",
       " ('even', 4),\n",
       " ('even', 6),\n",
       " ('even', 8),\n",
       " ('odd', 1),\n",
       " ('odd', 3),\n",
       " ('odd', 5),\n",
       " ('odd', 7),\n",
       " ('odd', 9),\n",
       " ('odd', 11)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numbers.map(lambda s: s[0]).collect())\n",
    "print('\\n')\n",
    "print(numbers.flatMap(lambda s: s[0]).collect())\n",
    "print('\\n')\n",
    "print(numbers.mapValues(lambda s: s**2).collect())\n",
    "print('\\n')\n",
    "numbers_with_list_as_value = sc.parallelize([('even', [0, 2, 4, 6, 8]), ('odd', [1, 3, 5, 7, 9]), ('odd', [11])])\n",
    "numbers_with_list_as_value.flatMapValues(lambda s: s).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='blue'>  Word count in text file </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oak is strong and also gives shade.',\n",
       " 'Cats and dogs each hate the other.',\n",
       " 'The Pipe began to rush whicle new.',\n",
       " \"Open the crate but don't break the glass.\",\n",
       " 'Add the sum to the product of these three.',\n",
       " 'Thieves who rob friends deserves jail.',\n",
       " 'The ripe taste of cheese improves with age.',\n",
       " 'Act on these orders with great speed.',\n",
       " 'The hog crawled under the high frence.',\n",
       " 'Move the vat over the hot fire.s']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtpath = 'input.txt'\n",
    "data = sc.textFile(txtpath, minPartitions=20)\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lkjsfmnalsdfasdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation from the words. This applies to every punctuation mark regardless its position.\n",
    "a = '.?lk-jsf./.m./.nalsdf!asdf!'\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "a.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lkafj'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_marks = ['.', '?', '-']\n",
    "\n",
    "def remove_punctuation_mark(string, marks_list):\n",
    "    for mark in marks_list:\n",
    "        if string.endswith(mark):\n",
    "            return string[:-1]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "test_string = 'lkafj?'\n",
    "remove_punctuation_mark(test_string, punctuation_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 11), ('to', 2), ('and', 2), ('of', 2), ('with', 2), ('these', 2), ('hate', 1), ('ripe', 1), ('pipe', 1), ('hot', 1), ('add', 1), ('cheese', 1), ('high', 1), ('gives', 1), ('who', 1), ('orders', 1), ('dogs', 1), ('crate', 1), ('crawled', 1), ('move', 1), ('is', 1), ('deserves', 1), ('product', 1), ('hog', 1), ('jail', 1), ('over', 1), ('vat', 1), ('each', 1), ('rush', 1), ('break', 1), ('on', 1), ('strong', 1), ('began', 1), ('open', 1), ('rob', 1), ('act', 1), ('frence', 1), (\"don't\", 1), ('taste', 1), ('other', 1), ('but', 1), ('oak', 1), ('also', 1), ('friends', 1), ('improves', 1), ('fire.s', 1), ('age', 1), ('great', 1), ('shade', 1), ('new', 1), ('glass', 1), ('three', 1), ('thieves', 1), ('under', 1), ('cats', 1), ('whicle', 1), ('speed', 1), ('sum', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.flatMap(lambda s: s.split()).map(lambda w: (remove_punctuation_mark(w.lower(), punctuation_marks), 1))\\\n",
    "       .reduceByKey(lambda x, y: x+y).collect(),\\\n",
    "       key=lambda n: n[1], reverse=True ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union / Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('even', 10), ('odd', 21), ('even', 20)])\n",
    "y = sc.parallelize([('even', 0), ('even', 20), ('odd', 1), ('even', 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('even', (0, 10)),\n",
       " ('even', (0, 20)),\n",
       " ('even', (20, 10)),\n",
       " ('even', (20, 20)),\n",
       " ('even', (20, 10)),\n",
       " ('even', (20, 20)),\n",
       " ('odd', (1, 21))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.join(x).collect() # Join every element of y to every element of x based on the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('even', 0),\n",
       " ('even', 20),\n",
       " ('odd', 1),\n",
       " ('even', 20),\n",
       " ('even', 10),\n",
       " ('odd', 21),\n",
       " ('even', 20)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.union(x).collect() # Union of the two RDDs. If there is a common element in the two RDDs, it is included twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "There are two ways to query the DataFrame.\n",
    "\n",
    "- \n",
    "```\n",
    "df = session.createDataFrame(data)\n",
    "df.createOrReplaceTempView('name')\n",
    "```\n",
    "\n",
    "- If we have already created a spark DataFrame through `.toDF`, and we want to query the dataframe we first need to register it, e.g. `df.registerTempView('name')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/thanasissdr/Downloads/sample.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o499.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/thanasissdr/Downloads/sample.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a7b325ee0f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/thanasissdr/Downloads/sample.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/thanasissdr/Downloads/sample.csv;'"
     ]
    }
   ],
   "source": [
    "filepath = '/home/thanasissdr/Downloads/sample.csv'\n",
    "\n",
    "df = session.read.csv(filepath, sep=',', header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|policyID|     county|\n",
      "+--------+-----------+\n",
      "|  119736|CLAY COUNTY|\n",
      "|  448094|CLAY COUNTY|\n",
      "|  206893|CLAY COUNTY|\n",
      "|  333743|CLAY COUNTY|\n",
      "|  172534|CLAY COUNTY|\n",
      "+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['policyID', 'county']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             county|count|\n",
      "+-------------------+-----+\n",
      "|  MIAMI DADE COUNTY| 4315|\n",
      "|     BROWARD COUNTY| 3193|\n",
      "|  PALM BEACH COUNTY| 2791|\n",
      "|       DUVAL COUNTY| 1894|\n",
      "|      ORANGE COUNTY| 1811|\n",
      "|    PINELLAS COUNTY| 1774|\n",
      "|        POLK COUNTY| 1629|\n",
      "|     VOLUSIA COUNTY| 1367|\n",
      "|HILLSBOROUGH COUNTY| 1166|\n",
      "|      MARION COUNTY| 1138|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('county').groupby('county').count().orderBy('count', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|blood_type|age|\n",
      "+----------+---+\n",
      "|         a| 10|\n",
      "|         b|  2|\n",
      "|         a|  2|\n",
      "|         a| 20|\n",
      "|         b| 10|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize([('a', 10), ('b', 2), ('a', 2), ('a', 20), ('b', 10)])\n",
    "\n",
    "small_df = session.createDataFrame(data, ['BLOOD_TYPE', 'AGE'])\n",
    "small_df = small_df.toDF('blood_type', 'age')\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|blood_type|age|\n",
      "+----------+---+\n",
      "|         b|  2|\n",
      "|         a|  2|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df.where(small_df.age < 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|blood_type|count|\n",
      "+----------+-----+\n",
      "|         b|    2|\n",
      "|         a|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df.groupBy('blood_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+\n",
      "|blood_type|age|squared_age|\n",
      "+----------+---+-----------+\n",
      "|         a| 10|      100.0|\n",
      "|         b|  2|        4.0|\n",
      "|         a|  2|        4.0|\n",
      "|         a| 20|      400.0|\n",
      "|         b| 10|      100.0|\n",
      "+----------+---+-----------+\n",
      "\n",
      "+----------+----------+\n",
      "|blood_type|new_column|\n",
      "+----------+----------+\n",
      "|         a|        10|\n",
      "|         b|         2|\n",
      "|         a|         2|\n",
      "|         a|        20|\n",
      "|         b|        10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "double = lambda s: s**2\n",
    "small_df.withColumn('squared_age', double(small_df.age)).show()\n",
    "small_df.withColumnRenamed('age', 'new_column').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')\n",
    "small_df.createOrReplaceTempView('small_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|policyID|     county|\n",
      "+--------+-----------+\n",
      "|  119736|CLAY COUNTY|\n",
      "|  448094|CLAY COUNTY|\n",
      "|  206893|CLAY COUNTY|\n",
      "|  333743|CLAY COUNTY|\n",
      "|  172534|CLAY COUNTY|\n",
      "+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT policyID, county FROM df'\n",
    "session.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|             county|   c|\n",
      "+-------------------+----+\n",
      "|  MIAMI DADE COUNTY|4315|\n",
      "|     BROWARD COUNTY|3193|\n",
      "|  PALM BEACH COUNTY|2791|\n",
      "|       DUVAL COUNTY|1894|\n",
      "|      ORANGE COUNTY|1811|\n",
      "|    PINELLAS COUNTY|1774|\n",
      "|        POLK COUNTY|1629|\n",
      "|     VOLUSIA COUNTY|1367|\n",
      "|HILLSBOROUGH COUNTY|1166|\n",
      "|      MARION COUNTY|1138|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT county, COUNT(county) as `c` FROM df GROUP BY county ORDER BY c DESC LIMIT 10'\n",
    "session.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying rdd operations \n",
    "We can apply map/filter into a DataFrame  by applying \n",
    "`DataFrame.rdd.map` or `DataFrame.rdd.filter` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 4, 4, 20]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'SELECT * FROM small_df WHERE small_df.age <= 12'\n",
    "teenagers = session.sql(query)\n",
    "teenagers.rdd.map(lambda s: s['age']*2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|blood_type|age|\n",
      "+----------+---+\n",
      "|         a|100|\n",
      "|         b|  4|\n",
      "|         a|  4|\n",
      "|         b|100|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers.rdd.mapValues(lambda s: s**2).toDF(['blood_type', 'age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a `parquet` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists('sample.parquet'):\n",
    "    df.write.parquet('sample.qarquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|blood_type|age|\n",
      "+----------+---+\n",
      "|         a| 10|\n",
      "|         b|  2|\n",
      "|         a|  2|\n",
      "|         a| 20|\n",
      "|         b| 10|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = session.read.parquet('sample.parquet')\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying an operation to a DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|blood_type|   age|\n",
      "+----------+------+\n",
      "|         a|1000.0|\n",
      "|         b|   8.0|\n",
      "|         a|   8.0|\n",
      "|         a|8000.0|\n",
      "|         b|1000.0|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x**3\n",
    "\n",
    "small_df.withColumn('age', cube(small_df.age)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize(zip(cycle(['a', 'b', 'ab']), [int(i) for i in np.random.randint(low=1, high=30, size=20)], \\\n",
    "                        [int(j) for j in np.random.randint(low=1, high=100, size=20)]))\n",
    "cols = ['blood_type', 'age', 'days_to_birthday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------------+\n",
      "|blood_type|age|days_to_birthday|\n",
      "+----------+---+----------------+\n",
      "|         a| 25|               5|\n",
      "|         b| 25|              60|\n",
      "|        ab| 17|               7|\n",
      "|         a| 20|              93|\n",
      "|         b| 14|              70|\n",
      "|        ab| 26|              43|\n",
      "|         a| 10|              91|\n",
      "|         b| 22|              88|\n",
      "|        ab| 28|              89|\n",
      "|         a| 18|              98|\n",
      "|         b|  1|              64|\n",
      "|        ab| 22|              19|\n",
      "|         a| 23|              23|\n",
      "|         b| 27|               9|\n",
      "|        ab|  9|              81|\n",
      "|         a| 13|              25|\n",
      "|         b|  3|              31|\n",
      "|        ab| 19|              61|\n",
      "|         a| 20|              38|\n",
      "|         b| 19|              84|\n",
      "+----------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = numbers.toDF(cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new column in an existing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------------+---------------------+\n",
      "|blood_type|age|days_to_birthday|age_multiplied_by_ten|\n",
      "+----------+---+----------------+---------------------+\n",
      "|         a| 25|               5|                  250|\n",
      "|         b| 25|              60|                  250|\n",
      "|        ab| 17|               7|                  170|\n",
      "|         a| 20|              93|                  200|\n",
      "|         b| 14|              70|                  140|\n",
      "|        ab| 26|              43|                  260|\n",
      "|         a| 10|              91|                  100|\n",
      "|         b| 22|              88|                  220|\n",
      "|        ab| 28|              89|                  280|\n",
      "|         a| 18|              98|                  180|\n",
      "|         b|  1|              64|                   10|\n",
      "|        ab| 22|              19|                  220|\n",
      "|         a| 23|              23|                  230|\n",
      "|         b| 27|               9|                  270|\n",
      "|        ab|  9|              81|                   90|\n",
      "|         a| 13|              25|                  130|\n",
      "|         b|  3|              31|                   30|\n",
      "|        ab| 19|              61|                  190|\n",
      "|         a| 20|              38|                  200|\n",
      "|         b| 19|              84|                  190|\n",
      "+----------+---+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiply_by_ten = udf(lambda s: s*10)\n",
    "extended_df = df.withColumn('age_multiplied_by_ten', multiply_by_ten(df.age))\n",
    "extended_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating `mean`, `std` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.05\n",
      "0.12297463417053223\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(df.select('age').rdd.map(lambda s: s[0]).mean())\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "#print(df.rdd.map(lambda s: s['age']).stdev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.05\n",
      "0.10369205474853516\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(df.rdd.map(lambda s: s['age']).mean())\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[avg(age): double]\n",
      "0.009303569793701172\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(df.select(mean(\"age\")))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
